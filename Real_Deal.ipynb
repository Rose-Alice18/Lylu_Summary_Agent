{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c50a14-70ca-429f-a36b-11e57589bbba",
   "metadata": {},
   "source": [
    "# The real deal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33c861d7-effe-43c9-959b-2557b47e8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, TypedDict\n",
    "import uuid\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import wave\n",
    "\n",
    "import sounddevice as sd\n",
    "import assemblyai as aai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END, START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50042ab-3055-4e2d-87ed-a33f334de7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration - Load API Keys from environment\n",
    "ASSEMBLYAI_API_KEY = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify keys are loaded\n",
    "if not ASSEMBLYAI_API_KEY:\n",
    "    raise ValueError(\"ASSEMBLYAI_API_KEY not found in environment variables\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "\n",
    "# Set up AssemblyAI\n",
    "aai.settings.api_key = ASSEMBLYAI_API_KEY\n",
    "\n",
    "# Audio configuration\n",
    "SAMPLE_RATE = 16000\n",
    "CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49dd5549-d31a-48a4-9563-0766286afeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionState(TypedDict):\n",
    "    \"\"\"State for the transcription and summarization process\"\"\"\n",
    "    session_id: str\n",
    "    audio_file_path: Optional[str]\n",
    "    raw_transcript: str\n",
    "    speaker_segments: List[Dict]\n",
    "    final_transcript: str\n",
    "    summary_report: Optional[str]\n",
    "    error_message: Optional[str]\n",
    "    processing_complete: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dea8884-65c2-417a-9606-3be44c1f1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRecorder:\n",
    "    \"\"\"Reliable audio recorder for AssemblyAI\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.is_recording = False\n",
    "        self.audio_data = []\n",
    "        \n",
    "    def start_recording(self):\n",
    "        \"\"\"Start recording audio\"\"\"\n",
    "        self.is_recording = True\n",
    "        self.audio_data = []\n",
    "        \n",
    "        def audio_callback(indata, frames, time, status):\n",
    "            if status:\n",
    "                print(f\"Audio status: {status}\")\n",
    "            if self.is_recording:\n",
    "                self.audio_data.append(indata.copy())\n",
    "        \n",
    "        try:\n",
    "            self.stream = sd.InputStream(\n",
    "                samplerate=SAMPLE_RATE,\n",
    "                channels=CHANNELS,\n",
    "                callback=audio_callback,\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            self.stream.start()\n",
    "            print(\"üé§ Recording started...\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to start recording: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def stop_recording(self):\n",
    "        \"\"\"Stop recording and create WAV file\"\"\"\n",
    "        self.is_recording = False\n",
    "        if hasattr(self, 'stream'):\n",
    "            self.stream.stop()\n",
    "            self.stream.close()\n",
    "        \n",
    "        if not self.audio_data:\n",
    "            print(\" No audio data recorded\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all audio chunks\n",
    "        full_audio = np.concatenate(self.audio_data, axis=0).flatten()\n",
    "        \n",
    "        # Convert to int16 (WAV standard)\n",
    "        audio_int16 = np.clip(full_audio * 32767, -32768, 32767).astype(np.int16)\n",
    "        \n",
    "        # Create WAV file using wave module (most compatible)\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\n",
    "        temp_filename = temp_file.name\n",
    "        temp_file.close()\n",
    "        \n",
    "        try:\n",
    "            with wave.open(temp_filename, 'wb') as wav_file:\n",
    "                wav_file.setnchannels(CHANNELS)\n",
    "                wav_file.setsampwidth(2)  # 2 bytes for int16\n",
    "                wav_file.setframerate(SAMPLE_RATE)\n",
    "                wav_file.writeframes(audio_int16.tobytes())\n",
    "            \n",
    "            # Verify the file\n",
    "            file_size = os.path.getsize(temp_filename)\n",
    "            duration = len(audio_int16) / SAMPLE_RATE\n",
    "            \n",
    "            print(f\" Audio saved: {temp_filename}\")\n",
    "            print(f\" File size: {file_size} bytes\")\n",
    "            print(f\" Duration: {duration:.1f} seconds\")\n",
    "            \n",
    "            return temp_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create WAV file: {e}\")\n",
    "            return None\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "453067a8-951b-40c4-90f8-c5187bee6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssemblyAITranscriber:\n",
    "    \"\"\"Transcriber using AssemblyAI SDK\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transcriber = aai.Transcriber()\n",
    "    \n",
    "    def transcribe_with_speakers(self, audio_file: str) -> tuple:\n",
    "        \"\"\"Transcribe audio with speaker diarization using AssemblyAI SDK\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üéµ Starting transcription...\")\n",
    "            \n",
    "            # Configure transcription with speaker labels\n",
    "            config = aai.TranscriptionConfig(\n",
    "                speaker_labels=True,\n",
    "                speakers_expected=None,  # Can be adjusted44\n",
    "                auto_chapters=False,\n",
    "                sentiment_analysis=False,\n",
    "                auto_highlights=False\n",
    "            )\n",
    "            \n",
    "            # Transcribe the audio file\n",
    "            print(\" Processing audio... This may take a moment...\")\n",
    "            transcript = self.transcriber.transcribe(audio_file, config)\n",
    "            \n",
    "            # Check if transcription was successful\n",
    "            if transcript.status == aai.TranscriptStatus.error:\n",
    "                print(f\" Transcription failed: {transcript.error}\")\n",
    "                return \"\", \"\", []\n",
    "            \n",
    "            # Get the full transcript text\n",
    "            full_text = transcript.text\n",
    "            print(\" Transcription completed!\")\n",
    "            \n",
    "            # Process speaker-labeled utterances\n",
    "            speaker_segments = []\n",
    "            formatted_transcript = \"\"\n",
    "            \n",
    "            if transcript.utterances:\n",
    "                print(f\" Processing {len(transcript.utterances)} speaker utterances...\")\n",
    "                \n",
    "                for utterance in transcript.utterances:\n",
    "                    segment = {\n",
    "                        \"speaker\": f\"Speaker_{utterance.speaker}\",\n",
    "                        \"text\": utterance.text,\n",
    "                        \"confidence\": utterance.confidence,\n",
    "                        \"start\": utterance.start / 1000,  # Convert ms to seconds\n",
    "                        \"end\": utterance.end / 1000\n",
    "                    }\n",
    "                    speaker_segments.append(segment)\n",
    "                    formatted_transcript += f\"Speaker_{utterance.speaker}: {utterance.text}\\n\\n\"\n",
    "                    \n",
    "                    print(f\" Speaker_{utterance.speaker}: {utterance.text[:100]}...\")\n",
    "            else:\n",
    "                # Fallback if no speaker labels detected\n",
    "                print(\" No speaker labels detected, treating as single speaker\")\n",
    "                speaker_segments = [{\n",
    "                    \"speaker\": \"Speaker_A\",\n",
    "                    \"text\": full_text,\n",
    "                    \"confidence\": 0.8,\n",
    "                    \"start\": 0,\n",
    "                    \"end\": 60  # Approximate\n",
    "                }]\n",
    "                formatted_transcript = f\"Speaker_A: {full_text}\\n\\n\"\n",
    "            \n",
    "            unique_speakers = len(set(seg['speaker'] for seg in speaker_segments))\n",
    "            print(f\" Detected {unique_speakers} unique speaker(s)\")\n",
    "            print(f\" Generated {len(speaker_segments)} segments\")\n",
    "            \n",
    "            return full_text, formatted_transcript, speaker_segments\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Transcription error: {e}\")\n",
    "            return \"\", \"\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba2304cf-bbe1-461a-9c32-8fc99bd5a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(state: TranscriptionState) -> TranscriptionState:\n",
    "    \"\"\"Node: Record audio from microphone\"\"\"\n",
    "    \n",
    "    print(\"üé§ AUDIO RECORDING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    recorder = AudioRecorder()\n",
    "    \n",
    "    # Start recording\n",
    "    if not recorder.start_recording():\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": \"Failed to start audio recording\",\n",
    "            \"processing_complete\": True\n",
    "        }\n",
    "    \n",
    "    print(\"üéôÔ∏è Recording in progress...\")\n",
    "    print(\"üî¥ Press Enter when finished speaking\")\n",
    "    \n",
    "    # Wait for user to press Enter\n",
    "    try:\n",
    "        input()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Recording cancelled\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": \"Recording cancelled by user\",\n",
    "            \"processing_complete\": True\n",
    "        }\n",
    "    \n",
    "    # Stop recording and save file\n",
    "    audio_file = recorder.stop_recording()\n",
    "    \n",
    "    if not audio_file:\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": \"Failed to save audio recording\",\n",
    "            \"processing_complete\": True\n",
    "        }\n",
    "    \n",
    "    print(\"‚úÖ Recording completed and saved\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"audio_file_path\": audio_file,\n",
    "        \"error_message\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f601142-e90e-40e5-b8ab-bff06a158840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_speakers(state: TranscriptionState) -> TranscriptionState:\n",
    "    \"\"\"Node: Transcribe audio with speaker diarization using AssemblyAI SDK\"\"\"\n",
    "    \n",
    "    if state.get(\"error_message\"):\n",
    "        return state\n",
    "    \n",
    "    audio_file = state.get(\"audio_file_path\")\n",
    "    if not audio_file:\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": \"No audio file available for transcription\",\n",
    "            \"processing_complete\": True\n",
    "        }\n",
    "    \n",
    "    print(\"\\n TRANSCRIPTION WITH SPEAKER DIARIZATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    transcriber = AssemblyAITranscriber()\n",
    "    \n",
    "    # Transcribe with speaker diarization\n",
    "    raw_transcript, formatted_transcript, speaker_segments = transcriber.transcribe_with_speakers(audio_file)\n",
    "    \n",
    "    # Clean up temp file\n",
    "    try:\n",
    "        os.unlink(audio_file)\n",
    "        print(\" Temporary audio file cleaned up\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if not raw_transcript.strip():\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": \"No speech detected in audio\",\n",
    "            \"processing_complete\": True\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Transcription processing completed!\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"raw_transcript\": raw_transcript,\n",
    "        \"final_transcript\": formatted_transcript,\n",
    "        \"speaker_segments\": speaker_segments,\n",
    "        \"error_message\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6717159-d7cc-48da-811d-a1d3f9f490ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(state: TranscriptionState) -> TranscriptionState:\n",
    "    \"\"\"Node: Generate structured summary from transcript\"\"\"\n",
    "    \n",
    "    if state.get(\"error_message\"):\n",
    "        return state\n",
    "    \n",
    "    print(\"\\n GENERATING SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        client = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.3,\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "        \n",
    "        transcript = state.get(\"final_transcript\", \"\")\n",
    "        speaker_segments = state.get(\"speaker_segments\", [])\n",
    "        \n",
    "        if not transcript.strip():\n",
    "            return {\n",
    "                **state,\n",
    "                \"summary_report\": \"  CONVERSATION SUMMARY\\n\\n No transcript available to summarize.\",\n",
    "                \"processing_complete\": True\n",
    "            }\n",
    "        \n",
    "        system_message = SystemMessage(content=\"\"\"\n",
    "You are an expert conversation analyzer and an intelligent conversation summarizer. Create a comprehensive, structured summary of the provided conversation transcript that includes speaker diarization.\n",
    "\n",
    "**SPECIAL INSTRUCTION FOR MEDICAL CONVERSATIONS:**\n",
    "If the conversation appears to be a medical visit, consultation, or healthcare-related discussion, additionally provide a structured MEDICAL VISIT SUMMARY section using this format or you can improvise too:\n",
    "\n",
    " MEDICAL VISIT SUMMARY\n",
    "\n",
    " PATIENT INFORMATION\n",
    "- Patient Name: [Extract from conversation]\n",
    "- Date of Birth: [If mentioned]\n",
    "- Medical Record Number: [If mentioned]\n",
    "\n",
    " VISIT DETAILS\n",
    "- Date of Visit: [Extract or use transcript date]\n",
    "- Provider: [Doctor/healthcare provider name]\n",
    "- Location: [Clinic/hospital name if mentioned]\n",
    "- Visit Type: [Routine, follow-up, urgent, etc.]\n",
    "- Duration: [If determinable from conversation]\n",
    "\n",
    " REASON FOR VISIT\n",
    "- Primary: [Main reason for the visit]\n",
    "- Secondary: [Additional concerns or follow-ups]\n",
    "- Patient concerns: [Any symptoms or concerns mentioned]\n",
    "\n",
    " DIAGNOSES & CONDITIONS\n",
    "[List all medical conditions discussed with current status]\n",
    "1. [Condition name] - [Status: controlled/uncontrolled/new/resolved]\n",
    "2. [Additional conditions as discussed]\n",
    "\n",
    " MEDICATIONS DISCUSSED\n",
    "- Current medications: [List medications mentioned]\n",
    "- New prescriptions: [Any new medications prescribed]\n",
    "- Medication changes: [Any adjustments discussed]\n",
    "\n",
    " TESTS & PROCEDURES\n",
    "- Tests ordered: [Any lab work, imaging, etc.]\n",
    "- Results reviewed: [Any test results discussed]\n",
    "- Vital signs: [If mentioned in conversation]\n",
    "\n",
    " RECOMMENDATIONS & PLAN\n",
    "- Treatment plan: [Specific recommendations given]\n",
    "- Lifestyle modifications: [Diet, exercise, etc.]\n",
    "- Follow-up instructions: [When to return, what to monitor]\n",
    "\n",
    " FOLLOW-UP\n",
    "- Next appointment: [Date/timeframe if mentioned]\n",
    "- When to call: [Circumstances requiring contact]\n",
    "- Monitoring instructions: [Home monitoring, etc.]\n",
    "\n",
    " IMPORTANT NOTES\n",
    "- Allergies: [If discussed]\n",
    "- Emergency instructions: [If provided]\n",
    "- Patient questions: [Questions to address at next visit]\n",
    "\n",
    "\n",
    "You can also add this on to the report with the following sections:\n",
    "\n",
    " üìã CONVERSATION OVERVIEW\n",
    "- Duration and context\n",
    "- \n",
    "- Number of speakers identified. Now, identify the speakers and their roles if possible (e.g., interviewer/interviewee, doctor/patient, etc.)\n",
    "- Type of conversation (meeting, interview, discussion, etc.)\n",
    "\n",
    " üéØ KEY POINTS SUMMARY\n",
    "- Main topics discussed\n",
    "- Important decisions made\n",
    "- Critical information shared\n",
    "\n",
    " üë• INDIVIDUAL SPEAKER CONTRIBUTIONS\n",
    "For each speaker, provide:\n",
    "- Their main contributions\n",
    "- Key points they raised\n",
    "- Their role/perspective in the conversation\n",
    "\n",
    " üí° NOTABLE IDEAS & INSIGHTS\n",
    "- Creative or innovative ideas mentioned\n",
    "- Important insights or revelations\n",
    "- Unique perspectives shared\n",
    "\n",
    " ‚úÖ ACTION ITEMS & NEXT STEPS\n",
    "- Specific actions mentioned\n",
    "- Deadlines or timelines discussed\n",
    "- Follow-up items identified\n",
    "\n",
    " üìå IMPORTANT DETAILS\n",
    "- Names, dates, numbers mentioned\n",
    "- Resources or references cited\n",
    "- Contact information or links\n",
    "\n",
    "Format the summary professionally with clear headers and bullet points. Be concise but comprehensive.\n",
    "\n",
    "Requirements:\n",
    "- Use clear, professional language appropriate to the content\n",
    "- Maintain confidentiality (use generic terms instead of personal names when appropriate)\n",
    "- Include relevant information, main topics, key decisions, and action items\n",
    "- If the content is medical, use medical terminology; if it's business, use business language, etc.\n",
    "- Highlight any urgent or important information\n",
    "- If the content is unclear or contains no meaningful information, note this in the summary\n",
    "- Adapt the summary style to match the content type (medical, business, personal, educational, etc.)\n",
    "\n",
    "Format the summary with clear sections and bullet points where appropriate.\n",
    "\"\"\")\n",
    "        \n",
    "        # Prepare detailed speaker analysis\n",
    "        speaker_info = \"\"\n",
    "        if speaker_segments:\n",
    "            unique_speakers = set(seg[\"speaker\"] for seg in speaker_segments)\n",
    "            speaker_info = f\"\\n\\nSPEAKER ANALYSIS:\\n\"\n",
    "            speaker_info += f\"Total unique speakers detected: {len(unique_speakers)}\\n\"\n",
    "            speaker_info += f\"Total segments: {len(speaker_segments)}\\n\\n\"\n",
    "            \n",
    "            for speaker in unique_speakers:\n",
    "                speaker_segs = [seg for seg in speaker_segments if seg[\"speaker\"] == speaker]\n",
    "                total_words = sum(len(seg[\"text\"].split()) for seg in speaker_segs)\n",
    "                total_duration = sum(seg.get(\"end\", 0) - seg.get(\"start\", 0) for seg in speaker_segs)\n",
    "                avg_confidence = sum(seg.get(\"confidence\", 0) for seg in speaker_segs) / len(speaker_segs) if speaker_segs else 0\n",
    "                \n",
    "                speaker_info += f\"{speaker}:\\n\"\n",
    "                speaker_info += f\"  - {len(speaker_segs)} segments\\n\"\n",
    "                speaker_info += f\"  - ~{total_words} words\\n\"\n",
    "                speaker_info += f\"  - {total_duration:.1f}s total speaking time\\n\"\n",
    "                speaker_info += f\"  - {avg_confidence:.2f} avg confidence\\n\\n\"\n",
    "        \n",
    "        user_message = HumanMessage(content=f\"\"\"\n",
    "Please analyze and summarize this conversation transcript with speaker diarization:\n",
    "\n",
    "FULL TRANSCRIPT:\n",
    "{transcript}\n",
    "{speaker_info}\n",
    "\n",
    "Create a structured report following the format specified in your instructions.\n",
    "\"\"\")\n",
    "        \n",
    "        response = client.invoke([system_message, user_message])\n",
    "        summary_report = response.content\n",
    "        \n",
    "        print(\"‚úÖ Summary generated successfully!\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"summary_report\": summary_report,\n",
    "            \"processing_complete\": True,\n",
    "            \"error_message\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error generating summary: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"error_message\": f\"Summary generation error: {str(e)}\",\n",
    "            \"processing_complete\": True\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4aebd1f9-02bd-4941-b82f-20df7d4949ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(state: TranscriptionState) -> TranscriptionState:\n",
    "    \"\"\"Node: Display final results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" TRANSCRIPTION & SUMMARY COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if state.get(\"error_message\"):\n",
    "        print(f\" Error: {state['error_message']}\")\n",
    "        return state\n",
    "    \n",
    "    # Display speaker information\n",
    "    speaker_segments = state.get(\"speaker_segments\", [])\n",
    "    if speaker_segments:\n",
    "        unique_speakers = set(seg[\"speaker\"] for seg in speaker_segments)\n",
    "        print(f\" Speakers Detected: {len(unique_speakers)} ({', '.join(unique_speakers)})\")\n",
    "        print(f\" Total Segments: {len(speaker_segments)}\")\n",
    "        \n",
    "        # Show detailed speaker breakdown\n",
    "        for speaker in unique_speakers:\n",
    "            speaker_segs = [seg for seg in speaker_segments if seg[\"speaker\"] == speaker]\n",
    "            total_words = sum(len(seg[\"text\"].split()) for seg in speaker_segs)\n",
    "            total_time = sum(seg.get(\"end\", 0) - seg.get(\"start\", 0) for seg in speaker_segs)\n",
    "            print(f\"   {speaker}: {total_words} words, {total_time:.1f}s\")\n",
    "    \n",
    "    # Display transcript\n",
    "    print(f\"\\n FULL TRANSCRIPT:\")\n",
    "    print(\"-\" * 60)\n",
    "    transcript = state.get(\"final_transcript\", \"No transcript available\")\n",
    "    print(transcript)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n GENERATED SUMMARY REPORT:\")\n",
    "    print(\"-\" * 60)\n",
    "    summary = state.get(\"summary_report\", \"No summary available\")\n",
    "    print(summary)\n",
    "    \n",
    "    # Save to file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_id = state.get(\"session_id\", \"unknown\")\n",
    "    \n",
    "    filename = f\"assemblyai_transcript_summary_{timestamp}_{session_id[:8]}.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"ASSEMBLYAI TRANSCRIPTION & SUMMARY REPORT\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            f.write(f\"Session ID: {session_id}\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "            \n",
    "            if speaker_segments:\n",
    "                unique_speakers = set(seg[\"speaker\"] for seg in speaker_segments)\n",
    "                f.write(f\"Speakers: {len(unique_speakers)} ({', '.join(unique_speakers)})\\n\")\n",
    "                \n",
    "                # Detailed speaker stats\n",
    "                f.write(\"\\nSPEAKER STATISTICS:\\n\")\n",
    "                for speaker in unique_speakers:\n",
    "                    speaker_segs = [seg for seg in speaker_segments if seg[\"speaker\"] == speaker]\n",
    "                    total_words = sum(len(seg[\"text\"].split()) for seg in speaker_segs)\n",
    "                    total_time = sum(seg.get(\"end\", 0) - seg.get(\"start\", 0) for seg in speaker_segs)\n",
    "                    avg_conf = sum(seg.get(\"confidence\", 0) for seg in speaker_segs) / len(speaker_segs)\n",
    "                    f.write(f\"{speaker}: {total_words} words, {total_time:.1f}s, {avg_conf:.2f} confidence\\n\")\n",
    "            else:\n",
    "                f.write(\"Speakers: None detected\\n\")\n",
    "                \n",
    "            f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "            f.write(\"TRANSCRIPT:\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            f.write(transcript + \"\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "            f.write(\"SUMMARY REPORT:\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            f.write(summary + \"\\n\")\n",
    "            \n",
    "        print(f\"\\n Results saved to: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Could not save to file: {e}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"processing_complete\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a87686e2-0e99-44cd-9197-382be262168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transcription_graph():\n",
    "    \"\"\"Create the LangGraph workflow for transcription and summarization\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(TranscriptionState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"record\", record_audio)\n",
    "    workflow.add_node(\"transcribe\", transcribe_with_speakers)\n",
    "    workflow.add_node(\"summarize\", generate_summary)\n",
    "    workflow.add_node(\"display\", display_results)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"record\")\n",
    "    workflow.add_edge(\"record\", \"transcribe\")\n",
    "    workflow.add_edge(\"transcribe\", \"summarize\")\n",
    "    workflow.add_edge(\"summarize\", \"display\")\n",
    "    workflow.add_edge(\"display\", END)\n",
    "    \n",
    "    return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "928d2f75-c333-4b99-be6f-1b41dfd247ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\" ASSEMBLYAI SDK TRANSCRIPTION AGENT\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Features:\")\n",
    "    print(\"‚Ä¢ Record audio from microphone\")\n",
    "    print(\"‚Ä¢ Professional speaker diarization\")\n",
    "    print(\"‚Ä¢ High-accuracy transcription\")\n",
    "    print(\"‚Ä¢ Detailed speaker analysis\")\n",
    "    print(\"‚Ä¢ Automatic summary generation\")\n",
    "    print(\"‚Ä¢ Structured report output\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Verify API keys\n",
    "    if not ASSEMBLYAI_API_KEY or len(ASSEMBLYAI_API_KEY) < 20:\n",
    "        print(\" AssemblyAI API key appears to be invalid\")\n",
    "        print(\" Get your API key from: https://www.assemblyai.com/dashboard/\")\n",
    "        return\n",
    "    \n",
    "    if not OPENAI_API_KEY or not OPENAI_API_KEY.startswith(\"sk-\"):\n",
    "        print(\" OpenAI API key appears to be invalid\")\n",
    "        return\n",
    "    \n",
    "    print(\" API keys configured\")\n",
    "    \n",
    "    # Test audio devices\n",
    "    print(\"\\n Available audio devices:\")\n",
    "    try:\n",
    "        devices = sd.query_devices()\n",
    "        input_devices = [d for d in devices if d['max_input_channels'] > 0]\n",
    "        if input_devices:\n",
    "            for device in input_devices[:3]:\n",
    "                print(f\"  ‚úì {device['name']}\")\n",
    "        else:\n",
    "            print(\"   No input devices found!\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\" Could not query audio devices: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"session_id\": str(uuid.uuid4()),\n",
    "        \"audio_file_path\": None,\n",
    "        \"raw_transcript\": \"\",\n",
    "        \"speaker_segments\": [],\n",
    "        \"final_transcript\": \"\",\n",
    "        \"summary_report\": None,\n",
    "        \"error_message\": None,\n",
    "        \"processing_complete\": False\n",
    "    }\n",
    "    \n",
    "    # Create and run the graph\n",
    "    graph = create_transcription_graph()\n",
    "    \n",
    "    try:\n",
    "        final_state = graph.invoke(initial_state)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        if final_state.get(\"error_message\"):\n",
    "            print(\" PROCESSING COMPLETED WITH ERRORS\")\n",
    "            print(f\"Error: {final_state.get('error_message')}\")\n",
    "        else:\n",
    "            print(\" PROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"Session ID: {final_state.get('session_id')}\")\n",
    "        print(f\"Transcript Length: {len(final_state.get('final_transcript', ''))}\")\n",
    "        print(f\"Summary Generated: {'Yes' if final_state.get('summary_report') else 'No'}\")\n",
    "        \n",
    "        speaker_segments = final_state.get('speaker_segments', [])\n",
    "        unique_speakers = set(seg['speaker'] for seg in speaker_segments)\n",
    "        print(f\"Speakers Detected: {len(unique_speakers)}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Process interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38432b8f-0bf1-4816-9faf-ef4b11cf4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ASSEMBLYAI SDK TRANSCRIPTION AGENT\n",
      "======================================================================\n",
      "Features:\n",
      "‚Ä¢ Record audio from microphone\n",
      "‚Ä¢ Professional speaker diarization\n",
      "‚Ä¢ High-accuracy transcription\n",
      "‚Ä¢ Detailed speaker analysis\n",
      "‚Ä¢ Automatic summary generation\n",
      "‚Ä¢ Structured report output\n",
      "======================================================================\n",
      " API keys configured\n",
      "\n",
      " Available audio devices:\n",
      "  ‚úì Microsoft Sound Mapper - Input\n",
      "  ‚úì External Microphone (Realtek(R)\n",
      "  ‚úì Stereo Mix (Realtek(R) Audio)\n",
      "üé§ AUDIO RECORDING\n",
      "==================================================\n",
      "üé§ Recording started...\n",
      "üéôÔ∏è Recording in progress...\n",
      "üî¥ Press Enter when finished speaking\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Audio saved: C:\\Users\\HP\\AppData\\Local\\Temp\\tmpd6cnjiru.wav\n",
      " File size: 1475180 bytes\n",
      " Duration: 46.1 seconds\n",
      "‚úÖ Recording completed and saved\n",
      "\n",
      " TRANSCRIPTION WITH SPEAKER DIARIZATION\n",
      "==================================================\n",
      "üéµ Starting transcription...\n",
      " Processing audio... This may take a moment...\n",
      " Transcription completed!\n",
      " Processing 6 speaker utterances...\n",
      " Speaker_A: I said, you can't come to Nigeria. So do you think I cannot see that they place all their hearts int...\n",
      " Speaker_B: How are you looking for your gps? And you are, you are trying to tell the app what it should do for ...\n",
      " Speaker_A: How would you....\n",
      " Speaker_B: Put Dyson and say you just....\n",
      " Speaker_A: You just tap....\n",
      " Speaker_B: On the gps, it will just load and pick your location. So if you want GPS of here, you have to be her...\n",
      " Detected 2 unique speaker(s)\n",
      " Generated 6 segments\n",
      " Temporary audio file cleaned up\n",
      "\n",
      "‚úÖ Transcription processing completed!\n",
      "\n",
      " GENERATING SUMMARY\n",
      "==================================================\n",
      "‚úÖ Summary generated successfully!\n",
      "\n",
      "================================================================================\n",
      " TRANSCRIPTION & SUMMARY COMPLETE\n",
      "================================================================================\n",
      " Speakers Detected: 2 (Speaker_A, Speaker_B)\n",
      " Total Segments: 6\n",
      "   Speaker_A: 52 words, 19.9s\n",
      "   Speaker_B: 57 words, 15.2s\n",
      "\n",
      " FULL TRANSCRIPT:\n",
      "------------------------------------------------------------\n",
      "Speaker_A: I said, you can't come to Nigeria. So do you think I cannot see that they place all their hearts into what they've done? Can see why they did what they did. Out of everybody's video, this is the most interesting video I've seen. But here.\n",
      "\n",
      "Speaker_B: How are you looking for your gps? And you are, you are trying to tell the app what it should do for you.\n",
      "\n",
      "Speaker_A: How would you.\n",
      "\n",
      "Speaker_B: Put Dyson and say you just.\n",
      "\n",
      "Speaker_A: You just tap.\n",
      "\n",
      "Speaker_B: On the gps, it will just load and pick your location. So if you want GPS of here, you have to be here to get your GPS with.\n",
      "\n",
      "\n",
      "\n",
      " GENERATED SUMMARY REPORT:\n",
      "------------------------------------------------------------\n",
      "# CONVERSATION SUMMARY\n",
      "\n",
      "## üìã CONVERSATION OVERVIEW\n",
      "- **Duration and Context:** The conversation appears to be a casual discussion between two individuals about navigating using GPS technology and a specific video that caught the attention of Speaker A.\n",
      "- **Number of Speakers Identified:** 2\n",
      "  - **Speaker A:** Engaged in discussing the effort put into a video and GPS functionality.\n",
      "  - **Speaker B:** Focused on explaining how to use GPS effectively.\n",
      "- **Type of Conversation:** Informal discussion\n",
      "\n",
      "## üéØ KEY POINTS SUMMARY\n",
      "- **Main Topics Discussed:**\n",
      "  - The effort and interest in a specific video.\n",
      "  - The functionality and usability of GPS technology.\n",
      "- **Important Decisions Made:** None noted; the conversation is more exploratory and opinion-based.\n",
      "- **Critical Information Shared:** Speaker B provided insight into how GPS should be used effectively.\n",
      "\n",
      "## üë• INDIVIDUAL SPEAKER CONTRIBUTIONS\n",
      "\n",
      "### Speaker A\n",
      "- **Main Contributions:**\n",
      "  - Expressed admiration for the effort put into a video.\n",
      "  - Shared thoughts on the interest level of the video compared to others.\n",
      "- **Key Points Raised:**\n",
      "  - Highlighted the emotional investment in the video.\n",
      "  \n",
      "### Speaker B\n",
      "- **Main Contributions:**\n",
      "  - Provided a practical explanation of how to use GPS.\n",
      "  - Clarified the necessity of being at a location to get accurate GPS data.\n",
      "- **Key Points Raised:**\n",
      "  - Emphasized the importance of tapping on the GPS app to load and pick a location.\n",
      "\n",
      "## üí° NOTABLE IDEAS & INSIGHTS\n",
      "- Speaker A's appreciation for the video suggests a deeper engagement with content that resonates emotionally.\n",
      "- Speaker B's explanation indicates a common misunderstanding about GPS usage, highlighting the need for clearer instructions for users.\n",
      "\n",
      "## ‚úÖ ACTION ITEMS & NEXT STEPS\n",
      "- No specific action items or next steps were identified in this informal conversation.\n",
      "\n",
      "## üìå IMPORTANT DETAILS\n",
      "- **Names, Dates, Numbers Mentioned:** None\n",
      "- **Resources or References Cited:** None\n",
      "- **Contact Information or Links:** None\n",
      "\n",
      "This summary captures the essence of the conversation, highlighting the main points discussed and the contributions of each speaker. The dialogue reflects an informal exchange of thoughts on video content and GPS technology.\n",
      "\n",
      " Results saved to: assemblyai_transcript_summary_20250717_103356_78fcb21e.txt\n",
      "\n",
      "============================================================\n",
      " PROCESSING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Session ID: 78fcb21e-dee0-4673-af8b-3a2213c33e29\n",
      "Transcript Length: 597\n",
      "Summary Generated: Yes\n",
      "Speakers Detected: 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47beaff9-7ab0-4199-a64d-7136d37ebd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
